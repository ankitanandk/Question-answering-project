{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘squad’: File exists\n",
      "--2021-11-27 22:35:24--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
      "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.110.153, 185.199.111.153, 185.199.109.153, ...\n",
      "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.110.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 42123633 (40M) [application/json]\n",
      "Saving to: ‘squad/train-v2.0.json’\n",
      "\n",
      "100%[======================================>] 42,123,633  78.5MB/s   in 0.5s   \n",
      "\n",
      "2021-11-27 22:35:25 (78.5 MB/s) - ‘squad/train-v2.0.json’ saved [42123633/42123633]\n",
      "\n",
      "--2021-11-27 22:35:25--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
      "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
      "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4370528 (4.2M) [application/json]\n",
      "Saving to: ‘squad/dev-v2.0.json’\n",
      "\n",
      "100%[======================================>] 4,370,528   --.-K/s   in 0.1s    \n",
      "\n",
      "2021-11-27 22:35:25 (33.4 MB/s) - ‘squad/dev-v2.0.json’ saved [4370528/4370528]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir squad\n",
    "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O squad/train-v2.0.json\n",
    "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O squad/dev-v2.0.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def read_squad(path):\n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "\n",
    "    return contexts, questions, answers\n",
    "\n",
    "train_contexts, train_questions, train_answers = read_squad('squad/train-v2.0.json')\n",
    "val_contexts, val_questions, val_answers = read_squad('squad/dev-v2.0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # sometimes squad answers are off by a character or two – fix this\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 1\n",
    "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
    "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 2\n",
    "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
    "\n",
    "add_end_idx(train_answers, train_contexts)\n",
    "add_end_idx(val_answers, val_contexts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(val_encodings, val_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForQuestionAnswering\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 3.938\n",
      "[1,   200] loss: 2.556\n",
      "[1,   300] loss: 1.881\n",
      "[1,   400] loss: 1.743\n",
      "[1,   500] loss: 1.586\n",
      "[1,   600] loss: 1.581\n",
      "[1,   700] loss: 1.486\n",
      "[1,   800] loss: 1.473\n",
      "[1,   900] loss: 1.394\n",
      "[1,  1000] loss: 1.372\n",
      "[1,  1100] loss: 1.380\n",
      "[1,  1200] loss: 1.398\n",
      "[1,  1300] loss: 1.339\n",
      "[1,  1400] loss: 1.368\n",
      "[1,  1500] loss: 1.356\n",
      "[1,  1600] loss: 1.294\n",
      "[1,  1700] loss: 1.278\n",
      "[1,  1800] loss: 1.270\n",
      "[1,  1900] loss: 1.266\n",
      "[1,  2000] loss: 1.216\n",
      "[1,  2100] loss: 1.272\n",
      "[1,  2200] loss: 1.220\n",
      "[1,  2300] loss: 1.232\n",
      "[1,  2400] loss: 1.242\n",
      "[1,  2500] loss: 1.239\n",
      "[1,  2600] loss: 1.187\n",
      "[1,  2700] loss: 1.203\n",
      "[2,   100] loss: 0.902\n",
      "[2,   200] loss: 0.907\n",
      "[2,   300] loss: 0.917\n",
      "[2,   400] loss: 0.890\n",
      "[2,   500] loss: 0.930\n",
      "[2,   600] loss: 0.905\n",
      "[2,   700] loss: 0.911\n",
      "[2,   800] loss: 0.914\n",
      "[2,   900] loss: 0.919\n",
      "[2,  1000] loss: 0.896\n",
      "[2,  1100] loss: 0.934\n",
      "[2,  1200] loss: 0.920\n",
      "[2,  1300] loss: 0.937\n",
      "[2,  1400] loss: 0.914\n",
      "[2,  1500] loss: 0.939\n",
      "[2,  1600] loss: 0.945\n",
      "[2,  1700] loss: 0.906\n",
      "[2,  1800] loss: 0.938\n",
      "[2,  1900] loss: 0.925\n",
      "[2,  2000] loss: 0.914\n",
      "[2,  2100] loss: 0.945\n",
      "[2,  2200] loss: 0.923\n",
      "[2,  2300] loss: 0.931\n",
      "[2,  2400] loss: 0.974\n",
      "[2,  2500] loss: 0.953\n",
      "[2,  2600] loss: 0.934\n",
      "[2,  2700] loss: 0.960\n",
      "[3,   100] loss: 0.626\n",
      "[3,   200] loss: 0.643\n",
      "[3,   300] loss: 0.619\n",
      "[3,   400] loss: 0.630\n",
      "[3,   500] loss: 0.671\n",
      "[3,   600] loss: 0.664\n",
      "[3,   700] loss: 0.635\n",
      "[3,   800] loss: 0.677\n",
      "[3,   900] loss: 0.636\n",
      "[3,  1000] loss: 0.681\n",
      "[3,  1100] loss: 0.650\n",
      "[3,  1200] loss: 0.659\n",
      "[3,  1300] loss: 0.661\n",
      "[3,  1400] loss: 0.642\n",
      "[3,  1500] loss: 0.698\n",
      "[3,  1600] loss: 0.709\n",
      "[3,  1700] loss: 0.691\n",
      "[3,  1800] loss: 0.671\n",
      "[3,  1900] loss: 0.681\n",
      "[3,  2000] loss: 0.714\n",
      "[3,  2100] loss: 0.685\n",
      "[3,  2200] loss: 0.695\n",
      "[3,  2300] loss: 0.709\n",
      "[3,  2400] loss: 0.719\n",
      "[3,  2500] loss: 0.673\n",
      "[3,  2600] loss: 0.734\n",
      "[3,  2700] loss: 0.720\n"
     ]
    }
   ],
   "source": [
    "## PYTORCH CODE\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "loss_values = []\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    for i ,batch in enumerate(train_loader,0):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "            \n",
    "        \n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "                \n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 100))\n",
    "                loss_values.append(running_loss /100)\n",
    "                running_loss = 0.0\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Trainset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print('Finished Training Trainset')\n",
    "            \n",
    "plt.plot(np.array(loss_values), 'r')\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './zimin1st.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: While BSkyB had been excluded from being a part of the ONdigital consortium, thereby making them a competitor by default, BSkyB was able to join ITV Digital's free-to-air replacement, Freeview, in which it holds an equal stake with the BBC, ITV, Channel 4 and National Grid Wireless. Prior to October 2005, three BSkyB channels were available on this platform: Sky News, Sky Three, and Sky Sports News. Initially BSkyB provided Sky Travel to the service. However, this was replaced by Sky Three on 31 October 2005, which was itself later re-branded as 'Pick TV' in 2011.\n",
      "question: What was Sky Travel later rebranded as?\n",
      "ANS: {'text': 'Pick TV', 'answer_start': 553, 'answer_end': 560}\n"
     ]
    }
   ],
   "source": [
    "# val_dataset[1300]\n",
    "# batch['input_ids']\n",
    "number = 1450\n",
    "print('context:',val_contexts[number])\n",
    "print('question:', val_questions[number])\n",
    "print('ANS:',val_answers[number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: Pick TV\n",
      "Prediction: ' pick tv\n"
     ]
    }
   ],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "# for i ,batch in enumerate(val_loader,0):\n",
    "#         # optim.zero_grad()\n",
    "#         input_ids = batch['input_ids']\n",
    "#         attention_mask = batch['attention_mask']\n",
    "#         start_positions = batch['start_positions']\n",
    "#         end_positions = batch['end_positions']\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "#         answer_start = torch.argmax(outputs[1])\n",
    "#         answer_end = torch.argmax(outputs[2]) +1\n",
    "#         answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "number = 1450\n",
    "batch = val_dataset[number]\n",
    "input_ids = batch['input_ids']\n",
    "# attention_mask = batch['attention_mask']\n",
    "# start_positions = batch['start_positions']\n",
    "# end_positions = batch['end_positions']\n",
    "context = val_contexts[number]\n",
    "question = val_questions[number]\n",
    "inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "# outputs = model(input_ids, attention_mask)\n",
    "outputs = model(**inputs)\n",
    "# outputs\n",
    "answer_start = torch.argmax(outputs[0])\n",
    "answer_end = torch.argmax(outputs[1]) +1\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
    "# answer = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end])\n",
    "print('Ground truth:', val_answers[number]['text'])\n",
    "print('Prediction:',answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: July 2013\n",
      "Prediction: july 2013\n"
     ]
    }
   ],
   "source": [
    "number = 1500\n",
    "batch = val_dataset[number]\n",
    "input_ids = batch['input_ids']\n",
    "context = val_contexts[number]\n",
    "question = val_questions[number]\n",
    "inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "# outputs = model(input_ids, attention_mask)\n",
    "outputs = model(**inputs)\n",
    "# outputs\n",
    "answer_start = torch.argmax(outputs[0])\n",
    "answer_end = torch.argmax(outputs[1]) +1\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
    "# answer = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end])\n",
    "print('Ground truth:', val_answers[number]['text'])\n",
    "print('Prediction:',answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DeepNLP]",
   "language": "python",
   "name": "conda-env-DeepNLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
